{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1750422080815,"user":{"displayName":"Sipan Hoj","userId":"06369710097250863993"},"user_tz":-120},"id":"uMxMMhA5YfaJ","outputId":"83d2db20-28b1-4351-9a78-65e60e0a9ccd"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# drive.mount('/content/drive')\n","drive.mount('/content/drive', force_remount=True)\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1754493730487,"user":{"displayName":"Sipan Hoj","userId":"06369710097250863993"},"user_tz":-120},"id":"T8Lq3drLK5Wg","outputId":"a6d28081-51e1-40a5-c51f-12f4c5c2dd2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["import os\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHDibM8-bYD_"},"outputs":[],"source":["!pip install -q pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hILhSfT4cPHv"},"outputs":[],"source":["!pip install -q bitsandbytes transformers accelerate datasets peft"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHAirEv6Paud"},"outputs":[],"source":["!pip install -q torch==2.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKv7YWzxE0zt"},"outputs":[],"source":["!pip install -q python-multipart fastapi uvicorn nest-asyncio pyngrok python-dotenv ftfy lxml_html_clean newspaper3k huggingface_hub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1750422090475,"user":{"displayName":"Sipan Hoj","userId":"06369710097250863993"},"user_tz":-120},"id":"Ns-GeJ_qcHLY","outputId":"cac44654-bd58-49af-f98d-2e5dd78d4fb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available!\n","GPU Name: NVIDIA L4\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    print(\"GPU is available!\")\n","    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"GPU not available. Using CPU.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1750422090752,"user":{"displayName":"Sipan Hoj","userId":"06369710097250863993"},"user_tz":-120},"id":"1ZIgoQ7DcsCM","outputId":"28deb28e-9980-4ed6-a232-89f53dd5bfca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Jun 20 12:21:28 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n","| N/A   46C    P8             16W /   72W |       3MiB /  23034MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1750422090763,"user":{"displayName":"Sipan Hoj","userId":"06369710097250863993"},"user_tz":-120},"id":"BFEPRwwZP3JS","outputId":"c8ad317b-6938-4fc1-d887-25448e0427e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}],"source":["import os\n","from dotenv import load_dotenv\n","load_dotenv(\"drive/MyDrive/google_colab/MT/secrets.env\")\n","# authtoken = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n","# print(authtoken)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1scqM3WE5Va"},"outputs":[],"source":["# # to download a model from hugging face\n","\n","# from huggingface_hub import login\n","# login(os.getenv(\"HUGGINGFACE_TOKEN\"))\n","\n","# from huggingface_hub import snapshot_download\n","\n","# local_dir = \"drive/MyDrive/google_colab/MT/models/granite-3.3-8b-instruct\"\n","\n","# snapshot_download(\n","#     repo_id=\"ibm-granite/granite-3.3-8b-instruct\",\n","#     local_dir=local_dir,\n","#     local_dir_use_symlinks=False,\n","#     token=True\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9QvDKg6k_85"},"outputs":[],"source":["import re\n","import ftfy\n","import unicodedata\n","import pandas as pd\n","from newspaper import Article\n","\n","def is_valid_email(email: str) -> bool:\n","    \"\"\"Validate email format using regex.\"\"\"\n","    email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n","    return re.match(email_pattern, email) is not None\n","\n","def clean_text(text):\n","    if not text or not text.strip():  # Check for None, empty, or whitespace-only input\n","        return text  # Return as is (or return an empty string if preferred)\n","\n","    # Fix text encoding issues\n","    text = ftfy.fix_text(text)\n","\n","    # Normalize unicode characters (e.g., fix apostrophes, ellipses, etc.)\n","    text = unicodedata.normalize(\"NFC\", text)\n","\n","    return text\n","\n","def extract_article(url):\n","    article = Article(url)\n","    article.download()\n","    article.parse()\n","    return article.title, article.text\n","\n","\n","def data_pre_processing(folder_name, filename):\n","    file_location = os.path.join(folder_name, filename)\n","    data_v1 = pd.read_csv(file_location)\n","    data_v2 = data_v1[['url', 'title', 'content', 'lang']]\n","\n","    exception_counter = 0\n","    for index, entity in data_v2.iterrows():\n","        try:\n","            title, content = extract_article(entity['url'])\n","            data_v2.at[index, \"extracted_title\"] = clean_text(title)\n","            data_v2.at[index, \"extracted_content\"] = clean_text(content)\n","            data_v2.at[index, \"title\"] = clean_text(data_v2.at[index, \"title\"])\n","            data_v2.at[index, \"content\"] = clean_text(data_v2.at[index, \"content\"])\n","            data_v2.at[index, \"content_extracted\"] = True\n","            data_v2.at[index, \"error\"] = \"\"\n","        except Exception as e:\n","            exception_counter += 1\n","            data_v2.at[index, \"content_extracted\"] = False\n","            data_v2.at[index, \"error\"] = f\"{e}\"\n","            # print(f'----------------------------------------------------------------')\n","\n","    print(f\"Number of exceptions: {exception_counter}/{len(data_v2)}\")\n","    print(f\"% of exceptions: {exception_counter*100/len(data_v2)} %\")\n","\n","    data_v2.to_json(f\"{folder_name}/processed_data.json\", orient=\"records\", lines=True, force_ascii=False)\n","\n","def print_line():\n","    print(\"----------------------------------------------------\")\n","\n","def calcuate_execution_time(start_time):\n","    end_time = time.time()\n","    processing_time = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n","    print(f\"Processing time: {processing_time}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpXnilT5KsUU"},"outputs":[],"source":["questions_12 = [\n","      \"What is the overall sentiment regarding Switzerland? (Very negative, Negative, Neutral, Positive, Very positive)\",\n","      \"Which aspect of the natural dimension is discussed? (Nature Dimension not addressed, Landscape/scenery, Geography, Weather/climate, Preserved nature, Nature activities, Other aspect of Nature dimension)\",\n","      \"What is the sentiment toward the natural dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","      \"Which aspect of the functional dimension is discussed? (Functional Dimension not addressed, Education system, Science/innovation, Products, Economy, Infrastructure, Politics, Living/working conditions, Security, Other aspect of Functional dimension)\",\n","      \"What is the sentiment toward the functional dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","      \"Which aspect of the normative dimension is discussed? (Normative Dimension not addressed, Environmental protection, Freedom/human rights, Civil rights, International engagement, Ethical issues/scandals, Conflict avoidance, Tolerance/openness, Other aspect of Normative dimension)\",\n","      \"What is the sentiment toward the normative dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","      \"Which aspect of the cultural dimension is discussed? (Cultural Dimension not addressed, Sports, Food, Cultural offer, Personalities, Traditions, History, Cultural diversity, Other aspect of Culture dimension)\",\n","      \"What is the sentiment toward the cultural dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","      \"Does the article contain disinformation? (No disinformation type, False connection, False context, Misleading content, Fabricated content, Manipulated content, Other disinformation type)\",\n","      \"What disinformation technique is used, if any? (No disinformation technique, Ad hominem attack, Emotional language, False dichotomies, Incoherence, Scapegoating, Other disinformation technique)\",\n","      \"What is the article about in short?\"\n","    ]\n","\n","columns_12 = [\n","        'overall_sentiment',\n","        'nature_dimension',\n","        'nature_sentiment',\n","        'functional_dimension',\n","        'functional_sentiment',\n","        'normative_dimension',\n","        'normative_sentiment',\n","        'cultural_dimension',\n","        'cultural_sentiment',\n","        'disinformation_type',\n","        'disinformation_technique',\n","        'summary'\n","    ]\n","\n","\n","questions_6 = [\n","        \"What is the overall sentiment regarding Switzerland? (Very negative, Negative, Neutral, Positive, Very positive)\",\n","        \"What is the sentiment toward the natural dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","        \"What is the sentiment toward the functional dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","        \"What is the sentiment toward the normative dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","        \"What is the sentiment toward the cultural dimension? (Very negative, Negative, Neutral, Positive, Very positive, No Sentiment)\",\n","        \"Does the article contain disinformation? (No disinformation type, False connection, False context, Misleading content, Fabricated content, Manipulated content, Other disinformation type)\",\n","    ]\n","\n","columns_6 = [\n","        'overall_sentiment',\n","        'nature_sentiment',\n","        'functional_sentiment',\n","        'normative_sentiment',\n","        'cultural_sentiment',\n","        'disinformation_type',\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ApgCOPjL0R7"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","def load_model(model_path):\n","  tokenizer = AutoTokenizer.from_pretrained(model_path)\n","  tokenizer.pad_token = tokenizer.eos_token\n","\n","  model = AutoModelForCausalLM.from_pretrained(\n","      model_path,\n","      device_map=\"auto\",\n","      torch_dtype=\"auto\",\n","      load_in_8bit=True\n","  )\n","\n","  return model, tokenizer\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPl-h_SHtXtZ"},"outputs":[],"source":["import re\n","def process_by_llama_3b(model, tokenizer, input_file_path, questions, columns, output_file_path):\n","\n","    data = pd.read_json(input_file_path, orient=\"records\", lines=True)\n","\n","    print(f\"Data len: {len(data)}\")\n","\n","    data['article_source'] = ''\n","    for col in columns:\n","        data[col] = ''\n","\n","    for index, row in data.iterrows():\n","        if not row['content_extracted'] or row['extracted_content'] == \"\" or len(row['content']) > len(row['extracted_content']):\n","            article = row['content']\n","            data.at[index, 'article_source'] = 'content'\n","        else:\n","            article = row['extracted_content']\n","            data.at[index, 'article_source'] = 'extracted_content'\n","\n","\n","        prompt = f\"\"\"\n","        You are given an article about Switzerland. Read it carefully.\n","\n","        ARTICLE:\n","        {article}\n","\n","        TASK:\n","        Answer the following multiple-choice questions based only on the information in the article.\n","\n","        Instructions:\n","        - Choose **only one** of the options provided for each question.\n","        - Write only the selected option.\n","        - **Do not** explain your answer.\n","\n","        QUESTIONS:\n","        \"\"\"\n","        for i, q in enumerate(questions, 1):\n","            prompt += f\"{i}. {q}\\n\"\n","\n","        # Tokenize and send to model\n","        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(model.device)\n","        with torch.no_grad():\n","            output = model.generate(\n","                **inputs,\n","                max_new_tokens=512,\n","                pad_token_id=tokenizer.eos_token_id,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_p=0.9,\n","                top_k=50\n","            )\n","\n","        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n","        response_text = decoded.replace(prompt, \"\").strip()\n","        # print(\"--------------------------------------------------\")\n","        # print(\"--------------------------------------------------\")\n","        # print(f\"Index: {index}\")\n","\n","        # print(f\"Response: {response_text}\")\n","        # print(\"--------------------------------------------------\")\n","        # print(\"--------------------------------------------------\")\n","\n","        answer_lines = re.findall(r\"^\\s*(\\d+)\\.\\s*(.*)$\", response_text, flags=re.MULTILINE)\n","        answers = {int(num): ans.strip() for num, ans in answer_lines}\n","\n","        # Fill in DataFrame columns\n","        for i, col in enumerate(columns, 1):\n","            data.at[index, col] = answers.get(i, \"\")\n","\n","\n","\n","    # Save updated DataFrame to CSV\n","    data.to_csv(output_file_path, index=False, encoding='utf-8')\n","    print(\"Processing complete. Saved to CSV.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqeU16uXcF8P"},"outputs":[],"source":["import re\n","import torch\n","import pandas as pd\n","\n","def process_by_granite(model, tokenizer, input_file_path, questions, columns, output_file_path):\n","    # Load input data\n","    data = pd.read_json(input_file_path, orient=\"records\", lines=True)\n","    print(f\"Data length: {len(data)}\")\n","\n","    # Prepare new columns\n","    data['article_source'] = ''\n","    for col in columns:\n","        data[col] = ''\n","\n","    for index, row in data.iterrows():\n","        # Select which article content to use\n","        if not row['content_extracted'] or row['extracted_content'] == \"\" or len(row['content']) > len(row['extracted_content']):\n","            article = row['content']\n","            data.at[index, 'article_source'] = 'content'\n","        else:\n","            article = row['extracted_content']\n","            data.at[index, 'article_source'] = 'extracted_content'\n","\n","        # Build prompt for Granite 3.3-8B-Instruct\n","        prompt = f\"\"\"\n","        You are given an article about Switzerland. Read it carefully.\n","\n","        ARTICLE:\n","        {article}\n","\n","        TASK:\n","        Answer the following multiple-choice questions based only on the information in the article.\n","\n","        Instructions:\n","        - Choose **only one** of the options provided for each question.\n","        - Write only the selected option.\n","        - **Do not** explain your answer.\n","\n","        QUESTIONS:\n","        \"\"\"\n","        for i, q in enumerate(questions, 1):\n","            prompt += f\"{i}. {q}\\n\"\n","\n","        # Tokenize and prepare inputs\n","        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(model.device)\n","\n","        with torch.no_grad():\n","            output = model.generate(\n","                **inputs,\n","                max_new_tokens=512,\n","                pad_token_id=tokenizer.eos_token_id,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_p=0.9,\n","                top_k=50\n","            )\n","\n","        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n","        response_text = decoded[len(prompt):].strip()\n","\n","        # print(f\"Index: {index}\")\n","        # print(\"--------------------------------------------------\")\n","\n","        # Extract answers\n","        answer_lines = re.findall(r\"^\\s*(\\d+)\\.\\s*(.*)$\", response_text, flags=re.MULTILINE)\n","        answers = {int(num): ans.strip() for num, ans in answer_lines}\n","\n","        # Fill answers into dataframe\n","        for i, col in enumerate(columns, 1):\n","            data.at[index, col] = answers.get(i, \"\")\n","\n","    # Save output\n","    data.to_csv(output_file_path, index=False, encoding='utf-8')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5Fqf9JA9bTo"},"outputs":[],"source":["import re\n","def process_by_mistral_7b(model, tokenizer, input_file_path, questions, columns, output_file_path):\n","\n","    data = pd.read_json(input_file_path, orient=\"records\", lines=True)\n","\n","    print(f\"Data len: {len(data)}\")\n","\n","    data['article_source'] = ''\n","    for col in columns:\n","        data[col] = ''\n","\n","    for index, row in data.iterrows():\n","        if not row['content_extracted'] or row['extracted_content'] == \"\" or len(row['content']) > len(row['extracted_content']):\n","            article = row['content']\n","            data.at[index, 'article_source'] = 'content'\n","        else:\n","            article = row['extracted_content']\n","            data.at[index, 'article_source'] = 'extracted_content'\n","\n","        prompt = f\"\"\"<s>[INST]\n","        You are given an article about Switzerland. Read it carefully.\n","\n","        ARTICLE:\n","        {article}\n","\n","        TASK:\n","        Answer the following multiple-choice questions based only on the information in the article.\n","\n","        Instructions:\n","        - Choose **only one** of the options provided for each question.\n","        - Write only the selected option.\n","        - **Do not** explain your answer.\n","\n","        QUESTIONS:\n","        \"\"\"\n","        for i, q in enumerate(questions, 1):\n","            prompt += f\"{i}. {q}\\n\"\n","\n","        prompt += \"[/INST]\"\n","\n","\n","        # Tokenize and send to model\n","        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(model.device)\n","        with torch.no_grad():\n","            output = model.generate(\n","                **inputs,\n","                max_new_tokens=512,\n","                pad_token_id=tokenizer.eos_token_id,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_p=0.9,\n","                top_k=50\n","            )\n","\n","        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n","        response_text = decoded.replace(prompt, \"\").strip()\n","        # print(\"--------------------------------------------------\")\n","        # print(\"--------------------------------------------------\")\n","        # print(f\"Index: {index}\")\n","\n","        # print(f\"Response: {response_text}\")\n","        # print(\"--------------------------------------------------\")\n","        # print(\"--------------------------------------------------\")\n","\n","        answer_lines = re.findall(r\"^\\s*(\\d+)\\.\\s*(.*)$\", response_text, flags=re.MULTILINE)\n","        answers = {int(num): ans.strip() for num, ans in answer_lines}\n","\n","        # Fill in DataFrame columns\n","        for i, col in enumerate(columns, 1):\n","            data.at[index, col] = answers.get(i, \"\")\n","\n","\n","\n","    # Save updated DataFrame to CSV\n","    data.to_csv(output_file_path, index=False, encoding='utf-8')\n","    print(\"Processing complete. Saved to CSV.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K5vohSS1oI3T"},"outputs":[],"source":["from datetime import datetime\n","data_count = 133\n","model, tokenizer = load_model(\"drive/MyDrive/google_colab/MT/models/Mistral-7B-Instruct-0.1\")\n","\n","print(\"Mistral-7B-Instruct-0.1\")\n","for i in range(5):\n","    print(f\"This is loop number {i + 1}\")\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","    print(timestamp)\n","    process_by_mistral_7b(model=model,\n","                  tokenizer=tokenizer,\n","                  input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","                  questions=questions_6,\n","                  columns=columns_6,\n","                  output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_mistral_7b_{data_count}_{timestamp}.csv\")\n","    print(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n","    print(\"--------------------------------------------------\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406,"referenced_widgets":["a069e244136848b1a3d716190b43119c","e514309195b1458e9c93a20d92405bc4","bb4ae8bda6034d5f8d79682f162dab6d","6b613e50fcaa491d85a03d1b3749c80e","3957f8cb4d73461088f0949f4bb1c4b3","8fce41f69e024cffbbc18571ba6c9d1a","c9c752f1cf1047bd8242fb6ea210e4ef","43d37b7a736e4fd0a8c59f48740ddd62","d1f3e84319834508ab9104647307f8ce","1cc1583648c040c397c30b3c2bc2c53c","5c27d7840bca4854adf95a4095508536"]},"id":"boLz1GpKNQsc","outputId":"e41d82b3-68dd-481c-d661-238395f7f814","executionInfo":{"status":"error","timestamp":1750422255269,"user_tz":-120,"elapsed":121381,"user":{"displayName":"Sipan Hoj","userId":"06369710097250863993"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a069e244136848b1a3d716190b43119c"}},"metadata":{}},{"output_type":"error","ename":"ValueError","evalue":"could not determine the shape of object type 'torch.storage.UntypedStorage'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-36-2542233281.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/google_colab/MT/models/Mistral-Nemo-Instruct-2407-pre\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mistral-Nemo-Instruct-2407\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-29-3927293312.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4572\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4573\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4574\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4575\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4576\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5018\u001b[0m             \u001b[0;31m# If shard_file is \"\", we use the existing state_dict instead of loading it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshard_file\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5020\u001b[0;31m                 state_dict = load_state_dict(\n\u001b[0m\u001b[1;32m   5021\u001b[0m                     \u001b[0mshard_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_quantized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5022\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'torch.storage.UntypedStorage'"]}],"source":["from datetime import datetime\n","data_count = 6\n","model, tokenizer = load_model(\"drive/MyDrive/google_colab/MT/models/Mistral-Nemo-Instruct-2407-pre\")\n","\n","print(\"Mistral-Nemo-Instruct-2407\")\n","for i in range(1):\n","    print(f\"This is loop number {i + 1}\")\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","    print(timestamp)\n","    process_by_mistral_7b(model=model,\n","                  tokenizer=tokenizer,\n","                  input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","                  questions=questions_6,\n","                  columns=columns_6,\n","                  output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_mistral_nemo_{data_count}_{timestamp}.csv\")\n","    print(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n","    print(\"--------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oq4bMuCToSyI"},"outputs":[],"source":["from datetime import datetime\n","data_count = 133\n","model, tokenizer = load_model(\"drive/MyDrive/google_colab/MT/models/Llama-3.2-3B-Instruct\")\n","\n","print(\"Llama-3.2-3B-Instruct\")\n","print(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n","for i in range(5):\n","    print(f\"This is loop number {i + 1}\")\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","    print(timestamp)\n","    process_by_llama_3b(\n","      model=model,\n","      tokenizer=tokenizer,\n","      input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","      questions=questions_6,\n","      columns=columns_6,\n","      output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_llama_3_2_3b_{data_count}_{timestamp}.csv\"\n","    )\n","    print(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n","    print(\"--------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jQhwQe-CKxKG"},"outputs":[],"source":["from datetime import datetime\n","data_count = 133\n","model, tokenizer = load_model(\"drive/MyDrive/google_colab/MT/models/granite-3.3-8b-instruct\")\n","\n","print(\"granite-3.3-8b-instruct\")\n","for i in range(3):\n","    print(f\"This is loop number {i + 1}\")\n","    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","    print(timestamp)\n","    process_by_granite(\n","      model=model,\n","      tokenizer=tokenizer,\n","      input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","      questions=questions_6,\n","      columns=columns_6,\n","      output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_granite_3_3_8b_{data_count}_{timestamp}.csv\"\n","    )\n","    print(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n","    print(\"--------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7IKsb-jEK5D"},"outputs":[],"source":["# model_mistral_nemo, tokenizer_mistral_nemo = load_model_2(\"drive/MyDrive/google_colab/MT/models/Mistral-Nemo-Instruct-FP8-2407\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["80f9680815f0445883ee7703bbc6a5df","da4041afb24f48099ef408d1199aa177","39be9f9f33a84eb5963885d7f35ff788","371dc205679f49c580b7ba2a816ae1e5","552cc502ca84435a939ea089660cc0d7","f5fbf40f692c40f9b478d5ad3c3b22cb","f598fc70165e4dc5b22f2f9fcbb38132","46d14aba6eac4e67b8d64f803696f2f1","de078e7ca24f499aa3e0f6f9f31a340f","691bd8c3ae97498e9cbbfb7368580026","defce1dca25f48eaac234930d5692cda"]},"executionInfo":{"elapsed":248914,"status":"ok","timestamp":1749687315963,"user":{"displayName":"Sipan Hoj","userId":"06369710097250863993"},"user_tz":-120},"id":"cfxPHX2pu4r-","outputId":"c9970bfe-0644-448b-fddc-a53aa6536a1e"},"outputs":[{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80f9680815f0445883ee7703bbc6a5df","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# model_llama_3_2_3b, tokenizer_llama_3_2_3b = load_model(\"drive/MyDrive/google_colab/MT/models/Llama-3.2-3B-Instruct\")\n","# model_mistral_7b, tokenizer_mistral_7b = load_model(\"drive/MyDrive/google_colab/MT/models/Mistral-Nemo-Instruct-2407\")\n","model_granite_3_3_8b, tokenizer_granite_3_3_8b = load_model(\"drive/MyDrive/google_colab/MT/models/granite-3.3-8b-instruct\")\n","# model_mistral_nemo, tokenizer_mistral_nemo = load_model(\"drive/MyDrive/google_colab/MT/models/Mistral-Nemo-Instruct-2407\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"666RH1ZpLFmC"},"outputs":[],"source":["from datetime import datetime\n","data_count = 133\n","# model_granite_3_3_8b, tokenizer_granite_3_3_8b = load_model(\"drive/MyDrive/google_colab/MT/models/granite-3.3-8b-instruct\")\n","\n","\n","for i in range(1):\n","    print(f\"This is loop number {i + 1}\")\n","    now = datetime.now()\n","    timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","    process_by_granite(\n","      model=model_granite_3_3_8b,\n","      tokenizer=tokenizer_granite_3_3_8b,\n","      input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","      questions=questions_6,\n","      columns=columns_6,\n","      output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_granite_3_3_8b_{data_count}_{timestamp}.csv\"\n","    )\n","\n","    # process_by_llama_3b(model=model_llama_3_2_3b,\n","    #                 tokenizer=tokenizer_llama_3_2_3b,\n","    #                 input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","    #                 questions=questions_6,\n","    #                 columns=columns_6,\n","    #                 output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_llama_3_2_3b_{data_count}_{timestamp}.csv\")\n","\n","\n","    # process_by_mistral_7b(model=model_mistral_7b,\n","    #               tokenizer=tokenizer_mistral_7b,\n","    #               input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","    #               questions=questions_6,\n","    #               columns=columns_6,\n","    #               output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_mistral_nemo_{data_count}_{timestamp}.csv\")\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"_QrHnLoYbepu","outputId":"cde51519-5d42-4508-cb4f-9b657be65684"},"outputs":[{"name":"stdout","output_type":"stream","text":["This is loop number 1\n","Data len: 133\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"]},{"name":"stdout","output_type":"stream","text":["Processing complete. Saved to CSV.\n"]}],"source":["# from datetime import datetime\n","# data_count = 133\n","# # model_llama_3_2_3b, tokenizer_llama_3_2_3b = load_model(\"drive/MyDrive/google_colab/MT/models/Llama-3.2-3B-Instruct\")\n","\n","\n","# for i in range(1):\n","#     print(f\"This is loop number {i + 1}\")\n","#     now = datetime.now()\n","#     timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n","#     process_by_llama_3b(model=model_llama_3_2_3b,\n","#                     tokenizer=tokenizer_llama_3_2_3b,\n","#                     input_file_path=f\"drive/MyDrive/google_colab/MT/uploads/processed_data_{data_count}.json\",\n","#                     questions=questions_6,\n","#                     columns=columns_6,\n","#                     output_file_path = f\"drive/MyDrive/google_colab/MT/uploads/final_file_llama_3_2_3b_{data_count}_{timestamp}.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnBeqC1xJnR7"},"outputs":[],"source":["# from transformers import pipeline\n","\n","# # Initialize the text generation pipeline\n","# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","# # Example prompt\n","# prompt = \"Please talk about Switzerland as a place to live.\"\n","\n","# # Generate text based on the prompt\n","# output = pipe(prompt, max_new_tokens=150, temperature=0.7)\n","\n","# # Print the generated text\n","# print(output[0][\"generated_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-_WWbxMHBUU"},"outputs":[],"source":["import os\n","import shutil\n","from fastapi.middleware.cors import CORSMiddleware\n","from fastapi.responses import JSONResponse\n","from dotenv import load_dotenv\n","from fastapi import FastAPI, UploadFile, File, Form\n","import time\n","\n","app = FastAPI()\n","\n","app.add_middleware(\n","    CORSMiddleware,\n","    # allow_origins=[\"http://localhost:4200\"],  # Allow only your frontend origin\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],  # Allows all HTTP methods (GET, POST, etc.)\n","    allow_headers=[\"*\"],  # Allows all headers\n",")\n","\n","@app.get(\"/\")\n","def health():\n","    start_time = time.time()\n","    print_line()\n","    calcuate_execution_time(start_time)\n","    return {\"health get\": \"I'm fine habibi.\"}\n","\n","@app.post(\"/\")\n","def health():\n","    start_time = time.time()\n","    print_line()\n","    calcuate_execution_time(start_time)\n","    return {\"health post\": \"I'm fine habibi.\"}\n","\n","@app.post(\"/process\")\n","async def process(\n","    file: UploadFile = File(...),\n","    model: str = Form(...)\n","    # email: str = Form(...)\n","):\n","    start_time = time.time()\n","    print_line()\n","    UPLOAD_FOLDER = \"drive/MyDrive/google_colab/MT/uploads\"\n","\n","    # Check if the uploaded file is a CSV\n","    if not file.filename.endswith('.csv'):\n","        return JSONResponse(content={\"Error\": \"Only CSV files are allowed!\"}, status_code=400)\n","\n","    if not os.path.exists(UPLOAD_FOLDER):\n","        os.makedirs(UPLOAD_FOLDER)\n","\n","    # Define the path where the file will be saved\n","    file_location = os.path.join(UPLOAD_FOLDER, file.filename)\n","\n","    # Save the uploaded file to the specified location\n","    with open(file_location, \"wb\") as buffer:\n","        shutil.copyfileobj(file.file, buffer)\n","\n","    try:\n","        data_pre_processing(folder_name=UPLOAD_FOLDER, filename=file.filename)\n","    except Exception as e:\n","        calcuate_execution_time(start_time)\n","        return JSONResponse(content={\"Error while pre-processing: \": str(e)}, status_code=500)\n","\n","\n","    calcuate_execution_time(start_time)\n","    return {\n","        \"filename\": file.filename,\n","        \"message\": \"The file has been uploaded.\",\n","        \"model\": model\n","    }\n","\n","    # try:\n","    #     print(f\"Model: {model}\")\n","    #     match model:\n","    #         case \"GPT-4o\":\n","    #             print(\"case 1\")\n","    #             process_by_gpt_4o(folder_name=UPLOAD_FOLDER, filename=file.filename)\n","    #         case \"Llama-3.2-3B-Instruct\":\n","    #             print(\"case 2\")\n","    #             process_by_llama_mini()\n","    #         case _:\n","    #             print(\"case 0\")\n","    #             return JSONResponse(content={f\"Unvalid model: {model}.\"}, status_code=500)\n","    # except Exception as e:\n","    #     return JSONResponse(content={f\"Error while processing using {model}: \": str(e)}, status_code=500)\n","\n","    # try:\n","    #     return {\n","    #         \"filename\": file.filename,\n","    #         \"message\": \"This is a test message.\",\n","    #         \"model\": model\n","    #     }\n","    # except Exception as e:\n","    #     return JSONResponse(content={\"error while returning the response: \": str(e)}, status_code=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2me6XQDaiDzA"},"outputs":[],"source":["    # folder_path = \"drive/MyDrive/google_colab/MT\"\n","    # items = os.listdir(folder_path)\n","\n","    # # Separate into files and directories\n","    # files = [f for f in items if os.path.isfile(os.path.join(folder_path, f))]\n","    # folders = [f for f in items if os.path.isdir(os.path.join(folder_path, f))]\n","\n","    # print(\"Files:\", files)\n","    # print(\"Folders:\", folders)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VixmBPdmHCTi"},"outputs":[],"source":["import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n","\n","# Allow nested event loops\n","nest_asyncio.apply()\n","\n","# Expose the app via ngrok\n","public_url = ngrok.connect(8000)\n","print(f\"Your app is live at: {public_url}\")\n","\n","# Run the server\n","uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fp1__Y0f2sT4"},"outputs":[],"source":["import time\n","\n","# Store the start time\n","start_time = time.time()\n","\n","# Simulate some processing (replace with your actual code)\n","time.sleep(5)  # Simulating 5 seconds of processing\n","\n","# Calculate the processing time and format it as hh:mm:ss\n","processing_time = time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))\n","\n","# Print the processing time\n","print(f\"Processing time: {processing_time}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOyLe+htlG8wHXBMSu/seSg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"371dc205679f49c580b7ba2a816ae1e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_691bd8c3ae97498e9cbbfb7368580026","placeholder":"​","style":"IPY_MODEL_defce1dca25f48eaac234930d5692cda","value":" 4/4 [03:53&lt;00:00, 49.75s/it]"}},"39be9f9f33a84eb5963885d7f35ff788":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46d14aba6eac4e67b8d64f803696f2f1","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de078e7ca24f499aa3e0f6f9f31a340f","value":4}},"46d14aba6eac4e67b8d64f803696f2f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"552cc502ca84435a939ea089660cc0d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"691bd8c3ae97498e9cbbfb7368580026":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80f9680815f0445883ee7703bbc6a5df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da4041afb24f48099ef408d1199aa177","IPY_MODEL_39be9f9f33a84eb5963885d7f35ff788","IPY_MODEL_371dc205679f49c580b7ba2a816ae1e5"],"layout":"IPY_MODEL_552cc502ca84435a939ea089660cc0d7"}},"da4041afb24f48099ef408d1199aa177":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5fbf40f692c40f9b478d5ad3c3b22cb","placeholder":"​","style":"IPY_MODEL_f598fc70165e4dc5b22f2f9fcbb38132","value":"Loading checkpoint shards: 100%"}},"de078e7ca24f499aa3e0f6f9f31a340f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"defce1dca25f48eaac234930d5692cda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f598fc70165e4dc5b22f2f9fcbb38132":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5fbf40f692c40f9b478d5ad3c3b22cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a069e244136848b1a3d716190b43119c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e514309195b1458e9c93a20d92405bc4","IPY_MODEL_bb4ae8bda6034d5f8d79682f162dab6d","IPY_MODEL_6b613e50fcaa491d85a03d1b3749c80e"],"layout":"IPY_MODEL_3957f8cb4d73461088f0949f4bb1c4b3"}},"e514309195b1458e9c93a20d92405bc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fce41f69e024cffbbc18571ba6c9d1a","placeholder":"​","style":"IPY_MODEL_c9c752f1cf1047bd8242fb6ea210e4ef","value":"Loading checkpoint shards:  20%"}},"bb4ae8bda6034d5f8d79682f162dab6d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_43d37b7a736e4fd0a8c59f48740ddd62","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1f3e84319834508ab9104647307f8ce","value":1}},"6b613e50fcaa491d85a03d1b3749c80e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cc1583648c040c397c30b3c2bc2c53c","placeholder":"​","style":"IPY_MODEL_5c27d7840bca4854adf95a4095508536","value":" 1/5 [01:46&lt;04:49, 72.31s/it]"}},"3957f8cb4d73461088f0949f4bb1c4b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fce41f69e024cffbbc18571ba6c9d1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9c752f1cf1047bd8242fb6ea210e4ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43d37b7a736e4fd0a8c59f48740ddd62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1f3e84319834508ab9104647307f8ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cc1583648c040c397c30b3c2bc2c53c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c27d7840bca4854adf95a4095508536":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}